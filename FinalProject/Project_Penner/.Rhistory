lotto <- lotto[-c(1,2)]
draw <- sample(1:length(lotto), 4, replace = F)
playoff <- lotto[draw]
draw <- sample(1:length(lotto), 4, replace = F)
playoff <- lotto[draw]
draw <- sample(1:length(lotto), 4, replace = F)
playoff <- lotto[draw]
draw <- sample(1:length(lotto), 4, replace = F)
playoff <- lotto[draw]
draw <- sample(1:length(lotto), 4, replace = F)
playoff <- lotto[draw]
318/9
x <- c(10.5, 23, 14, 35, 47, 13.5, 108, 43.5, 28, 90.0)
y <- c(10,14,12,16,20,12,22,16,14,12)
cov(x,y)
var(x)
var(y)
df1 <- data("iris")
data("iris")
df1 <- iris
n <- c(2:21)
sum(1000/1.1^n)
sum(1000/1.05^n)
n <- c(1:40)
sum(1000/1.05^n)
sum(75000/1.05^n)
n <- c(1:20)
k <- c(21:40)
sum(75000/1.05^n)
sum(100000/1.05^k)
934665.8+469687.6
1404353-1286931
install.packages(rvest)
install.packages("rvest")
library(rvest)
ballot <- read_html("https://ballotpedia.org/Virginia_House_of_Delegates_elections,_2017")
ballot %>%
html_node("td") %>%
html_text()
ballot <- read_html("https://ballotpedia.org/Virginia_House_of_Delegates_elections,_2017")
ballot %>%
html_node("td") %>%
html_table()
ballot %>%
html_nodes("td")
ballot %>%
html_nodes(".eln-reporting , .eln-vote-count , .eln-name")
%>% html_text()
ballot %>%
html_nodes(".eln-reporting , .eln-vote-count , .eln-name") %>%
html_text()
ballot %>%
html_nodes(".eln-reporting") %>%
html_text()
ballot %>%
html_nodes(".eln-reporting , .eln-vote-count , .eln-name") %>%
html_table()
test <- ballot %>%
html_nodes(".eln-reporting , .eln-vote-count , .eln-name") %>%
html_table()
test <- ballot %>%
html_nodes(".eln-reporting , .eln-vote-count , .eln-name")
ballot <- read_html("https://www.nytimes.com/elections/results/maine-ballot-measure-medicaid-expansion")
test <- ballot %>%
html_nodes(".eln-reporting , .eln-vote-count , .eln-name")
bball <- read_html("http://www.espn.com/mens-college-basketball/team/_/id/201/")
test <- bball %>%
html_nodes(".club-schedule li a")
test <- bball %>%
html_nodes(".club-schedule li a") %>%
html_table()
test <- bball %>%
html_nodes(".club-schedule li a") %>%
html_text()
scores <- bball %>%
html_nodes(".club-schedule li a") %>%
html_text()
?stringr
library(stringr)
?strsplit
score
scores
tweets_bball <- searchTwitter('OU Basketball', n=500)
install.packages("twitteR")
install.packages("ROAuth")
install.packages("httr")
library(twitteR)
library(ROAuth)
library(httr)
# Set API Keys
api_key <- "wMFyv90ZfsfOkvHnyYOLF5Nx3"
api_secret <- "t5ZTQdPGfjmMoFI2rjhJrFPbNPAo8lgFGrO0vfV9jz6OzEqsp2"
access_token <- "2306075156-6HRFQPgmAnke5NXz5W5zFzcbqaVAcxAkxQeLnJV"
access_token_secret <- "1icGG0de2NS7TVOkGXEwwibXO6U3afnNt1Aplttih7pCI"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
api_key <- "wMFyv90ZfsfOkvHnyYOLF5Nx3"
api_secret <- "t5ZTQdPGfjmMoFI2rjhJrFPbNPAo8lgFGrO0vfV9jz6OzEqsp2"
access_token <- "2306075156-6HRFQPgmAnke5NXz5W5zFzcbqaVAcxAkxQeLnJV"
access_token_secret <- "1icGG0de2NS7TVOkGXEwwibXO6U3afnNt1Aplttih7pCI"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets_bball <- searchTwitter('OU Basketball', n=500)
library(plyr)
feed_ball = laply(tweets_sanders, function(t) t$getText())
feed_ball = laply(tweets_bball, function(t) t$getText())
tweettable <- as.data.frame(tweets_bball)
tweets_bball$text
tweettable <- do.call("rbind", lapply(tweets_bball, as.data.frame))
install.packages("mlr")
install.packages("glmnet")
# PS9
# Alli Penner
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data")
names(housing) <- c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat","medv")
housing$lmedv    <- log(housing$medv)
housing$medv     <- NULL # drop median value
formula    <- as.formula(lmedv ~ .^3 +
poly(crim, 6) +
poly(zn, 6) +
poly(indus, 6) +
poly(nox, 6) +
poly(rm, 6) +
poly(age, 6) +
poly(dis, 6) +
poly(rad, 6) +
poly(tax, 6) +
poly(ptratio, 6) +
poly(b, 6) +
poly(lstat, 6))
mod_matrix <- data.frame(model.matrix(formula, housing))
#now replace the intercept column by the response since MLR will do
#"y ~ ." and get the intercept by default
mod_matrix[, 1] = housing$lmedv
colnames(mod_matrix)[1] = "lmedv" #make sure to rename it otherwise MLR won't find it
head(mod_matrix) #just make sure everything is hunky-dory
# Break up the data:
n <- nrow(mod_matrix)
train <- sample(n, size = .8*n)
test  <- setdiff(1:n, train)
housing.train <- mod_matrix[train,]
housing.test <- mod_matrix[test, ]
library(glmnet)
lassodata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# new prediction algorithm
LASSOModel <- makeLearner("regr.glmnet")
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
LASSOparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = LASSOModel,
task = lassodata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
LASSOModel <- setHyperPars(learner=LASSOModel, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(LASSOModel,lassodata,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = LASSOModel, task = lassodata)
?glmnet
# PS9
# Alli Penner
library(glmnet)
library(mlr)
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data")
names(housing) <- c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat","medv")
housing$lmedv    <- log(housing$medv)
housing$medv     <- NULL # drop median value
formula    <- as.formula(lmedv ~ .^3 +
poly(crim, 6) +
poly(zn, 6) +
poly(indus, 6) +
poly(nox, 6) +
poly(rm, 6) +
poly(age, 6) +
poly(dis, 6) +
poly(rad, 6) +
poly(tax, 6) +
poly(ptratio, 6) +
poly(b, 6) +
poly(lstat, 6))
mod_matrix <- data.frame(model.matrix(formula, housing))
#now replace the intercept column by the response since MLR will do
#"y ~ ." and get the intercept by default
mod_matrix[, 1] = housing$lmedv
colnames(mod_matrix)[1] = "lmedv" #make sure to rename it otherwise MLR won't find it
head(mod_matrix) #just make sure everything is hunky-dory
# Break up the data:
n <- nrow(mod_matrix)
train <- sample(n, size = .8*n)
test  <- setdiff(1:n, train)
housing.train <- mod_matrix[train,]
housing.test <- mod_matrix[test, ]
lassodata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# new prediction algorithm
LASSOModel <- makeLearner("regr.glmnet")
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
LASSOparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = LASSOModel,
task = lassodata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
LASSOModel <- setHyperPars(learner=LASSOModel, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(LASSOModel,lassodata,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = LASSOModel, task = lassodata)
# PS9
# Alli Penner
library(glmnet)
library(mlr)
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data")
names(housing) <- c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat","medv")
housing$lmedv    <- log(housing$medv)
housing$medv     <- NULL # drop median value
formula    <- as.formula(lmedv ~ .^3 +
poly(crim, 6) +
poly(zn, 6) +
poly(indus, 6) +
poly(nox, 6) +
poly(rm, 6) +
poly(age, 6) +
poly(dis, 6) +
poly(rad, 6) +
poly(tax, 6) +
poly(ptratio, 6) +
poly(b, 6) +
poly(lstat, 6))
mod_matrix <- data.frame(model.matrix(formula, housing))
#now replace the intercept column by the response since MLR will do
#"y ~ ." and get the intercept by default
mod_matrix[, 1] = housing$lmedv
colnames(mod_matrix)[1] = "lmedv" #make sure to rename it otherwise MLR won't find it
head(mod_matrix) #just make sure everything is hunky-dory
# Break up the data:
n <- nrow(mod_matrix)
train <- sample(n, size = .8*n)
test  <- setdiff(1:n, train)
housing.train <- mod_matrix[train,]
housing.test <- mod_matrix[test, ]
lassodata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# new prediction algorithm
LASSOModel <- makeLearner("regr.glmnet")
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
LASSOparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = LASSOModel,
task = lassodata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = LASSOparam,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
LASSOModel <- setHyperPars(learner=LASSOModel, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(LASSOModel,lassodata,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = LASSOModel, task = lassodata)
# PS9
# Alli Penner
library(glmnet)
library(mlr)
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data")
names(housing) <- c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat","medv")
housing$lmedv    <- log(housing$medv)
housing$medv     <- NULL # drop median value
formula    <- as.formula(lmedv ~ .^3 +
poly(crim, 6) +
poly(zn, 6) +
poly(indus, 6) +
poly(nox, 6) +
poly(rm, 6) +
poly(age, 6) +
poly(dis, 6) +
poly(rad, 6) +
poly(tax, 6) +
poly(ptratio, 6) +
poly(b, 6) +
poly(lstat, 6))
mod_matrix <- data.frame(model.matrix(formula, housing))
#now replace the intercept column by the response since MLR will do
#"y ~ ." and get the intercept by default
mod_matrix[, 1] = housing$lmedv
colnames(mod_matrix)[1] = "lmedv" #make sure to rename it otherwise MLR won't find it
head(mod_matrix) #just make sure everything is hunky-dory
# Break up the data:
n <- nrow(mod_matrix)
train <- sample(n, size = .8*n)
test  <- setdiff(1:n, train)
housing.train <- mod_matrix[train,]
housing.test <- mod_matrix[test, ]
lassodata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# new prediction algorithm
LASSOModel <- makeLearner("regr.glmnet")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
# Do the resampling
sampleResults <- resample(learner = predAlg, task = theTask, resampling = resampleStrat, measures=list(rmse))
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
LASSOparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = LASSOModel,
task = lassodata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = LASSOparam,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
LASSOModel <- setHyperPars(learner=LASSOModel, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(LASSOModel,lassodata,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = LASSOModel, task = lassodata)
prediction <- predict(finalModel, newdata = housing.test)
str(prediction)
performance(prediction)
performance(prediction,measures = list(rmse) )
# ridge regression
RIDGEModel <- makeLearner("regr.glmnet")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
ridgedata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# Do the resampling
sampleResults <- resample(learner = RIDGEmodel, task = ridgedata, resampling = resampleStrat, measures=list(rmse))
# Search over penalty parameter lambda and force elastic net parameter to be 1 (ridge)
RIDGEparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=0))
# Take 50 random guesses at lambda
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# tune
tunedModel <- tuneParams(learner = RIDGEModel,
task = ridgedata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = RIDGEparam,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
RIDGEModel <- setHyperPars(learner=RIDGEModel, par.vals = tunedModel$x)
# cross validates
resample(RIDGEModel,ridgedata,resampleStrat,measures=list(rmse))
# Train the final model
finallasso <- train(learner = RIDGEModel, task = ridgedata)
predictionridge <- predict(finalridge, newdata = housing.test)
performance(prediction)
RIDGEModel <- makeLearner("regr.glmnet")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
ridgedata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# Do the resampling
sampleResults <- resample(learner = RIDGEModel, task = ridgedata, resampling = resampleStrat, measures=list(rmse))
# Search over penalty parameter lambda and force elastic net parameter to be 1 (ridge)
RIDGEparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=0))
# Take 50 random guesses at lambda
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# tune
tunedModel <- tuneParams(learner = RIDGEModel,
task = ridgedata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = RIDGEparam,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
RIDGEModel <- setHyperPars(learner=RIDGEModel, par.vals = tunedModel$x)
# cross validates
resample(RIDGEModel,ridgedata,resampleStrat,measures=list(rmse))
# Train the final model
finallasso <- train(learner = RIDGEModel, task = ridgedata)
predictionridge <- predict(finalridge, newdata = housing.test)
performance(prediction,measures = list(rmse) )
str(prediction)
RIDGEModel <- makeLearner("regr.glmnet")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
ridgedata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# Do the resampling
sampleResults <- resample(learner = RIDGEModel, task = ridgedata, resampling = resampleStrat, measures=list(rmse))
# Search over penalty parameter lambda and force elastic net parameter to be 1 (ridge)
RIDGEparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=0))
# Take 50 random guesses at lambda
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# tune
tunedModel <- tuneParams(learner = RIDGEModel,
task = ridgedata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = RIDGEparam,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
RIDGEModel <- setHyperPars(learner=RIDGEModel, par.vals = tunedModel$x)
# cross validates
resample(RIDGEModel,ridgedata,resampleStrat,measures=list(rmse))
# Train the final model
finalridge <- train(learner = RIDGEModel, task = ridgedata)
predictionridge <- predict(finalridge, newdata = housing.test)
performance(prediction,measures = list(rmse) )
str(prediction)
performance(predictionridge,measures = list(rmse) )
elasticModel <- makeLearner("regr.glmnet")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
elasticdata <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
# Do the resampling
sampleResults <- resample(learner = elasticModel, task = elasticdata, resampling = resampleStrat, measures=list(rmse))
# Search over penalty parameter lambda and force elastic net parameter to be 1 (elastic)
elasticparam <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=1))
# Take 50 random guesses at lambda
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# tune
tunedModel <- tuneParams(learner = elasticModel,
task = elasticdata,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = elasticparam,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
elasticModel <- setHyperPars(learner=elasticModel, par.vals = tunedModel$x)
# cross validates
resample(elasticModel,elasticdata,resampleStrat,measures=list(rmse))
# Train the final model
finalelastic <- train(learner = elasticModel, task = elasticdata)
predictionelastic <- predict(finalelastic, newdata = housing.test)
performance(predictionelastic,measures = list(rmse) )
str(predictionelastic)
setwd("~/NFLProject")
# Data Science for Economists
# Analysis of NFL Wins
# By Alli Penner
# read data
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2009.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2010.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2011.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2012.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2013.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2014.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2015.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2016.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2017.csv")
# processing
setwd("~/NFLProject")
# Data Science for Economists
# Analysis of NFL Wins
# By Alli Penner
# read data
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2009.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2010.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2011.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2012.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2013.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2014.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2015.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2016.csv")
pbp2009 <- read.csv("~/nflscrapR-data/data/season_play_by_play/pbp_2017.csv")
# processing
# Data Science for Economists
# Analysis of NFL Wins
# By Alli Penner
# read data
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2009.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2010.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2011.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2012.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2013.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2014.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2015.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2016.csv")
pbp2009 <- read.csv("~\nflscrapR-data\data\season_play_by_play\pbp_2017.csv")
# processing
# Data Science for Economists
# Analysis of NFL Wins
# By Alli Penner
# read data
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2009.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2010.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2011.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2012.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2013.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2014.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2015.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2016.csv")
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2017.csv")
# processing
# Data Science for Economists
# Analysis of NFL Wins
# By Alli Penner
# read data
pbp2009 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2009.csv")
pbp2010 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2010.csv")
pbp2011 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2011.csv")
pbp2012 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2012.csv")
pbp2013 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2013.csv")
pbp2014 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2014.csv")
pbp2015 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2015.csv")
pbp2016 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2016.csv")
pbp2017 <- read.csv("~/NFLProject/nflscrapR-data/data/season_play_by_play/pbp_2017.csv")
# processing
View(pbp2016)
View(pbp2011)
View(pbp2011)
